{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "601e9cf0-cb99-4015-af99-5de7d928fd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enhanced_main_system.py\n",
    "# Complete OpenAI to Z Challenge Checkpoint 2 Solution\n",
    "# Multi-scale archaeological discovery with full compliance\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Import all enhanced components\n",
    "from simple_region_config import SimpleRegionConfig\n",
    "from enhanced_data_acquisition import EnhancedDataAcquisition\n",
    "from enhanced_data_processor import EnhancedDataProcessor\n",
    "from enhanced_ai_analyzer import EnhancedAIAnalyzer\n",
    "from enhanced_results_manager import EnhancedResultsManager\n",
    "\n",
    "class EnhancedAmazonArchaeology:\n",
    "    \"\"\"\n",
    "    Complete enhanced system for Amazon archaeological discovery\n",
    "    Implements multi-scale analysis with full Checkpoint 2 compliance\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the complete enhanced system\"\"\"\n",
    "        print(\"üèõÔ∏è Enhanced Amazon Archaeological Discovery System\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"üéØ OpenAI to Z Challenge - Checkpoint 2 Solution\")\n",
    "        print(\"üî¨ Multi-scale Network Detection: 50km ‚Üí 10km ‚Üí 2km\")\n",
    "        print(\"üìä Dual-source Analysis: Optical + Radar\")\n",
    "        print(\"ü§ñ AI-powered with Archaeological Knowledge\")\n",
    "        print(\"‚úÖ Full Checkpoint 2 Compliance\")\n",
    "        print()\n",
    "        \n",
    "        # Initialize all components\n",
    "        print(\"üîß Initializing system components...\")\n",
    "        \n",
    "        self.region_config = SimpleRegionConfig()\n",
    "        self.data_acquisition = EnhancedDataAcquisition()\n",
    "        self.data_processor = EnhancedDataProcessor()\n",
    "        self.ai_analyzer = EnhancedAIAnalyzer()\n",
    "        self.results_manager = EnhancedResultsManager()\n",
    "        \n",
    "        # Track pipeline progress\n",
    "        self.pipeline_status = {\n",
    "            'authentication': False,\n",
    "            'data_loaded': False,\n",
    "            'processing_complete': False,\n",
    "            'ai_analysis_complete': False,\n",
    "            'submission_ready': False\n",
    "        }\n",
    "        \n",
    "        # Store results from each stage\n",
    "        self.loaded_data = {}\n",
    "        self.processed_data = {}\n",
    "        self.ai_analyses = {}\n",
    "        self.final_submission = {}\n",
    "        \n",
    "        print(\"‚úÖ All components initialized successfully!\")\n",
    "        print()\n",
    "    \n",
    "    def step_1_setup_and_authentication(self) -> bool:\n",
    "        \"\"\"\n",
    "        Step 1: Setup Google Earth Engine authentication\n",
    "        \"\"\"\n",
    "        print(\"üîë STEP 1: Setup and Authentication\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Show available regions\n",
    "        print(\"üåç Available Amazon regions:\")\n",
    "        regions = self.region_config.get_regions_by_priority(8)\n",
    "        for region_id, info in regions.items():\n",
    "            priority_emoji = \"üî¥\" if info['priority'] == 'high' else \"üü°\" if info['priority'] == 'medium' else \"üü¢\"\n",
    "            sites_emoji = \"üèõÔ∏è\" if info['known_sites'] else \"üîç\"\n",
    "            print(f\"  {priority_emoji} {sites_emoji} {region_id}: {info['name']} ({info['country']})\")\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # Setup Google Earth Engine\n",
    "        if self.data_acquisition.setup_google_earth_engine():\n",
    "            self.pipeline_status['authentication'] = True\n",
    "            print(\"‚úÖ Step 1 completed - Authentication successful!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Step 1 failed - Please setup Google Earth Engine\")\n",
    "            print(\"\\nüìö Setup Instructions:\")\n",
    "            print(\"1. Visit: https://earthengine.google.com/\")\n",
    "            print(\"2. Sign up with Google account\")\n",
    "            print(\"3. Wait for approval (1-2 days)\")\n",
    "            print(\"4. Run: import ee; ee.Authenticate()\")\n",
    "            print(\"5. Re-run this system\")\n",
    "            return False\n",
    "    \n",
    "    def step_2_load_dual_source_data(self, max_regions: int = 3) -> bool:\n",
    "        \"\"\"\n",
    "        Step 2: Load dual-source satellite data\n",
    "        Implements Checkpoint 2 requirement for two independent sources\n",
    "        \"\"\"\n",
    "        if not self.pipeline_status['authentication']:\n",
    "            print(\"‚ùå Please complete Step 1 first!\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"üì° STEP 2: Load Dual-Source Data\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"üéØ Target regions: {max_regions}\")\n",
    "        print(\"üìä Data sources: Optical (Sentinel-2) + Radar (PALSAR/Sentinel-1)\")\n",
    "        print(\"‚úÖ Checkpoint 2 compliance: Two independent public sources\")\n",
    "        print()\n",
    "        \n",
    "        # Load data for selected regions\n",
    "        self.loaded_data = self.data_acquisition.load_multiple_regions(max_regions)\n",
    "        \n",
    "        if self.loaded_data:\n",
    "            self.pipeline_status['data_loaded'] = True\n",
    "            \n",
    "            # Show data summary\n",
    "            data_summary = self.data_acquisition.get_data_summary()\n",
    "            self.data_acquisition.show_summary()\n",
    "            \n",
    "            print(f\"\\n‚úÖ Step 2 completed - Data loaded for {len(self.loaded_data)} regions!\")\n",
    "            print(f\"üìä Checkpoint 2 compliance verified: ‚úÖ\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Step 2 failed - No data loaded\")\n",
    "            return False\n",
    "    \n",
    "    def step_3_multi_scale_processing(self) -> bool:\n",
    "        \"\"\"\n",
    "        Step 3: Multi-scale progressive image processing\n",
    "        Creates images for all three analysis scales\n",
    "        \"\"\"\n",
    "        if not self.pipeline_status['data_loaded']:\n",
    "            print(\"‚ùå Please complete Step 2 first!\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"üî¨ STEP 3: Multi-Scale Processing\")\n",
    "        print(\"-\" * 40)\n",
    "        print(\"üåç Scale 1: Regional network detection (50km)\")\n",
    "        print(\"üîç Scale 2: Zone site identification (10km)\")\n",
    "        print(\"üéØ Scale 3: Site feature confirmation (2km)\")\n",
    "        print()\n",
    "        \n",
    "        # Process all loaded regions with multi-scale analysis\n",
    "        self.processed_data = self.data_processor.process_all_regions(self.loaded_data)\n",
    "        \n",
    "        if self.processed_data:\n",
    "            self.pipeline_status['processing_complete'] = True\n",
    "            \n",
    "            # Count total discoveries\n",
    "            total_candidates = sum(\n",
    "                len(result['discovery_candidates']) \n",
    "                for result in self.processed_data.values()\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n‚úÖ Step 3 completed - Multi-scale processing finished!\")\n",
    "            print(f\"üéØ Discovery candidates found: {total_candidates}\")\n",
    "            print(f\"üìÅ Images created in: {self.data_processor.output_folder}/\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Step 3 failed - Processing unsuccessful\")\n",
    "            return False\n",
    "    \n",
    "    def step_4_ai_analysis_pipeline(self) -> bool:\n",
    "        \"\"\"\n",
    "        Step 4: Complete AI analysis pipeline\n",
    "        Regional ‚Üí Zone ‚Üí Site ‚Üí Leverage analysis\n",
    "        \"\"\"\n",
    "        if not self.pipeline_status['processing_complete']:\n",
    "            print(\"‚ùå Please complete Step 3 first!\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"ü§ñ STEP 4: AI Analysis Pipeline\")\n",
    "        print(\"-\" * 40)\n",
    "        print(\"üåç Stage 1: Regional network analysis\")\n",
    "        print(\"üîç Stage 2: Zone site detection\")\n",
    "        print(\"üéØ Stage 3: Site confirmation\")\n",
    "        print(\"üîÑ Stage 4: Discovery leverage\")\n",
    "        print()\n",
    "        \n",
    "        # Run complete multi-scale AI analysis\n",
    "        self.ai_analyses = self.ai_analyzer.analyze_all_scales(self.processed_data)\n",
    "        \n",
    "        if self.ai_analyses:\n",
    "            self.pipeline_status['ai_analysis_complete'] = True\n",
    "            \n",
    "            # Get all discoveries\n",
    "            all_discoveries = self.ai_analyzer.get_high_confidence_discoveries(min_confidence=0.5)\n",
    "            \n",
    "            print(f\"\\n‚úÖ Step 4 completed - AI analysis finished!\")\n",
    "            print(f\"üèõÔ∏è Archaeological discoveries: {len(all_discoveries)}\")\n",
    "            print(f\"üìù Prompts logged: {sum(len(prompts) if isinstance(prompts, list) else 1 for prompts in self.ai_analyses.values() if prompts)}\")\n",
    "            print(f\"üîÑ Leverage analysis: {'‚úÖ' if self.ai_analyses.get('leverage') else '‚ùå'}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Step 4 failed - AI analysis unsuccessful\")\n",
    "            return False\n",
    "    \n",
    "    def step_5_create_checkpoint2_submission(self) -> bool:\n",
    "        \"\"\"\n",
    "        Step 5: Create complete Checkpoint 2 submission\n",
    "        Validates all requirements and creates submission package\n",
    "        \"\"\"\n",
    "        if not self.pipeline_status['ai_analysis_complete']:\n",
    "            print(\"‚ùå Please complete Step 4 first!\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"üì¶ STEP 5: Create Checkpoint 2 Submission\")\n",
    "        print(\"-\" * 40)\n",
    "        print(\"‚úÖ Validating all Checkpoint 2 requirements...\")\n",
    "        print()\n",
    "        \n",
    "        # Get data summary and discoveries\n",
    "        data_summary = self.data_acquisition.get_data_summary()\n",
    "        all_discoveries = self.data_processor.get_all_discoveries()\n",
    "        \n",
    "        # Add AI-enhanced information to discoveries\n",
    "        ai_discoveries = self.ai_analyzer.get_high_confidence_discoveries(min_confidence=0.3)\n",
    "        \n",
    "        # Merge processor and AI discoveries\n",
    "        enhanced_discoveries = self.merge_discoveries(all_discoveries, ai_discoveries)\n",
    "        \n",
    "        # Create submission package\n",
    "        self.final_submission = self.results_manager.create_checkpoint2_submission(\n",
    "            data_summary, \n",
    "            self.ai_analyses, \n",
    "            enhanced_discoveries\n",
    "        )\n",
    "        \n",
    "        if self.final_submission:\n",
    "            self.pipeline_status['submission_ready'] = True\n",
    "            \n",
    "            # Save submission files\n",
    "            submission_file = self.results_manager.save_submission()\n",
    "            summary_file = self.results_manager.create_summary_report()\n",
    "            \n",
    "            # Show final summary\n",
    "            self.results_manager.show_final_summary()\n",
    "            \n",
    "            print(f\"\\n‚úÖ Step 5 completed - Submission package ready!\")\n",
    "            print(f\"üìÑ Files created:\")\n",
    "            print(f\"   ‚Ä¢ {submission_file}\")\n",
    "            print(f\"   ‚Ä¢ {summary_file}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Step 5 failed - Submission requirements not met\")\n",
    "            return False\n",
    "    \n",
    "    def merge_discoveries(self, processor_discoveries: List[Dict], ai_discoveries: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Merge discoveries from processor and AI analyzer\n",
    "        \"\"\"\n",
    "        merged = []\n",
    "        \n",
    "        # Start with processor discoveries (have spatial analysis)\n",
    "        for discovery in processor_discoveries:\n",
    "            # Find matching AI analysis\n",
    "            ai_match = None\n",
    "            for ai_disc in ai_discoveries:\n",
    "                # Simple proximity matching (in real implementation, use spatial analysis)\n",
    "                if (abs(discovery.get('center_lat', 0) - ai_disc.get('center_lat', 0)) < 0.01 and\n",
    "                    abs(discovery.get('center_lng', 0) - ai_disc.get('center_lng', 0)) < 0.01):\n",
    "                    ai_match = ai_disc\n",
    "                    break\n",
    "            \n",
    "            # Enhance discovery with AI information\n",
    "            if ai_match:\n",
    "                discovery['ai_confidence'] = ai_match.get('confidence_score', 0)\n",
    "                discovery['ai_analysis'] = ai_match.get('ai_response', '')\n",
    "                discovery['confidence'] = max(discovery.get('confidence', 0), ai_match.get('confidence_score', 0))\n",
    "            \n",
    "            merged.append(discovery)\n",
    "        \n",
    "        # Add any AI discoveries not matched\n",
    "        for ai_disc in ai_discoveries:\n",
    "            if not any(abs(discovery.get('center_lat', 0) - ai_disc.get('center_lat', 0)) < 0.01 and\n",
    "                      abs(discovery.get('center_lng', 0) - ai_disc.get('center_lng', 0)) < 0.01\n",
    "                      for discovery in merged):\n",
    "                merged.append(ai_disc)\n",
    "        \n",
    "        # Sort by confidence\n",
    "        return sorted(merged, key=lambda x: x.get('confidence', 0), reverse=True)\n",
    "    \n",
    "    def run_complete_pipeline(self, max_regions: int = 3) -> bool:\n",
    "        \"\"\"\n",
    "        Run the complete Checkpoint 2 pipeline automatically\n",
    "        \"\"\"\n",
    "        print(f\"üöÄ COMPLETE CHECKPOINT 2 PIPELINE\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"üìä Analyzing {max_regions} regions\")\n",
    "        print(f\"üéØ Target: Full Checkpoint 2 compliance\")\n",
    "        print(f\"‚è±Ô∏è Estimated time: 15-30 minutes\")\n",
    "        print()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Authentication\n",
    "            if not self.step_1_setup_and_authentication():\n",
    "                return False\n",
    "            \n",
    "            print(f\"\\n{'='*60}\")\n",
    "            \n",
    "            # Step 2: Data loading\n",
    "            if not self.step_2_load_dual_source_data(max_regions):\n",
    "                return False\n",
    "            \n",
    "            print(f\"\\n{'='*60}\")\n",
    "            \n",
    "            # Step 3: Multi-scale processing\n",
    "            if not self.step_3_multi_scale_processing():\n",
    "                return False\n",
    "            \n",
    "            print(f\"\\n{'='*60}\")\n",
    "            \n",
    "            # Step 4: AI analysis\n",
    "            if not self.step_4_ai_analysis_pipeline():\n",
    "                return False\n",
    "            \n",
    "            print(f\"\\n{'='*60}\")\n",
    "            \n",
    "            # Step 5: Submission creation\n",
    "            if not self.step_5_create_checkpoint2_submission():\n",
    "                return False\n",
    "            \n",
    "            # Success!\n",
    "            elapsed_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"\\nüéâ COMPLETE SUCCESS!\")\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"‚úÖ All 5 steps completed successfully\")\n",
    "            print(f\"üèõÔ∏è Archaeological analysis finished\")\n",
    "            print(f\"üì¶ Checkpoint 2 submission ready\")\n",
    "            print(f\"‚è±Ô∏è Total time: {elapsed_time/60:.1f} minutes\")\n",
    "            print()\n",
    "            \n",
    "            # Final validation summary\n",
    "            self.show_final_validation_summary()\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Pipeline failed: {e}\")\n",
    "            print(\"üîß Check error messages above for troubleshooting\")\n",
    "            return False\n",
    "    \n",
    "    def run_step_by_step_mode(self):\n",
    "        \"\"\"\n",
    "        Interactive step-by-step mode\n",
    "        Allows running one step at a time with user control\n",
    "        \"\"\"\n",
    "        print(f\"üìã STEP-BY-STEP INTERACTIVE MODE\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"üéØ Run each step individually with full control\")\n",
    "        print()\n",
    "        \n",
    "        while True:\n",
    "            self.show_pipeline_status()\n",
    "            \n",
    "            print(f\"\\nüöÄ Available Steps:\")\n",
    "            print(f\"1. Setup and authentication {'‚úÖ' if self.pipeline_status['authentication'] else '‚è∏Ô∏è'}\")\n",
    "            print(f\"2. Load dual-source data {'‚úÖ' if self.pipeline_status['data_loaded'] else '‚è∏Ô∏è'}\")\n",
    "            print(f\"3. Multi-scale processing {'‚úÖ' if self.pipeline_status['processing_complete'] else '‚è∏Ô∏è'}\")\n",
    "            print(f\"4. AI analysis pipeline {'‚úÖ' if self.pipeline_status['ai_analysis_complete'] else '‚è∏Ô∏è'}\")\n",
    "            print(f\"5. Create submission {'‚úÖ' if self.pipeline_status['submission_ready'] else '‚è∏Ô∏è'}\")\n",
    "            print(f\"6. Run all remaining steps\")\n",
    "            print(f\"7. Show detailed status\")\n",
    "            print(f\"0. Exit\")\n",
    "            \n",
    "            try:\n",
    "                choice = input(f\"\\nSelect step (0-7): \").strip()\n",
    "                \n",
    "                if choice == '0':\n",
    "                    break\n",
    "                elif choice == '1':\n",
    "                    self.step_1_setup_and_authentication()\n",
    "                elif choice == '2':\n",
    "                    regions = input(\"How many regions to analyze? (1-5, default: 3): \").strip()\n",
    "                    max_regions = int(regions) if regions.isdigit() and 1 <= int(regions) <= 5 else 3\n",
    "                    self.step_2_load_dual_source_data(max_regions)\n",
    "                elif choice == '3':\n",
    "                    self.step_3_multi_scale_processing()\n",
    "                elif choice == '4':\n",
    "                    self.step_4_ai_analysis_pipeline()\n",
    "                elif choice == '5':\n",
    "                    self.step_5_create_checkpoint2_submission()\n",
    "                elif choice == '6':\n",
    "                    # Run all remaining steps\n",
    "                    regions = input(\"How many regions for complete analysis? (1-5, default: 3): \").strip()\n",
    "                    max_regions = int(regions) if regions.isdigit() and 1 <= int(regions) <= 5 else 3\n",
    "                    \n",
    "                    if not self.pipeline_status['authentication']:\n",
    "                        self.step_1_setup_and_authentication()\n",
    "                    if not self.pipeline_status['data_loaded']:\n",
    "                        self.step_2_load_dual_source_data(max_regions)\n",
    "                    if not self.pipeline_status['processing_complete']:\n",
    "                        self.step_3_multi_scale_processing()\n",
    "                    if not self.pipeline_status['ai_analysis_complete']:\n",
    "                        self.step_4_ai_analysis_pipeline()\n",
    "                    if not self.pipeline_status['submission_ready']:\n",
    "                        self.step_5_create_checkpoint2_submission()\n",
    "                    break\n",
    "                elif choice == '7':\n",
    "                    self.show_detailed_status()\n",
    "                else:\n",
    "                    print(\"‚ùå Invalid choice, please try again\")\n",
    "                    \n",
    "                input(\"\\nPress Enter to continue...\")\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\\nüëã Exiting step-by-step mode\")\n",
    "                break    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error: {e}\")\n",
    "                input(\"Press Enter to continue...\")\n",
    "    \n",
    "    def show_pipeline_status(self):\n",
    "        \"\"\"Show current pipeline status\"\"\"\n",
    "        print(f\"\\nüìä PIPELINE STATUS:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        for step, completed in self.pipeline_status.items():\n",
    "            status = \"‚úÖ Complete\" if completed else \"‚è∏Ô∏è Pending\"\n",
    "            step_name = step.replace('_', ' ').title()\n",
    "            print(f\"{step_name}: {status}\")\n",
    "        \n",
    "        if self.loaded_data:\n",
    "            print(f\"\\nData: {len(self.loaded_data)} regions loaded\")\n",
    "        if self.processed_data:\n",
    "            total_candidates = sum(len(result.get('discovery_candidates', [])) for result in self.processed_data.values())\n",
    "            print(f\"Processing: {total_candidates} candidates found\")\n",
    "        if hasattr(self, 'ai_analyses') and self.ai_analyses:\n",
    "            discoveries = len(self.ai_analyzer.get_high_confidence_discoveries(0.3))\n",
    "            print(f\"AI Analysis: {discoveries} discoveries\")\n",
    "    \n",
    "    def show_detailed_status(self):\n",
    "        \"\"\"Show detailed system status\"\"\"\n",
    "        print(f\"\\nüìã DETAILED SYSTEM STATUS\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Component status\n",
    "        print(\"üîß Components:\")\n",
    "        print(f\"   Region Config: {len(self.region_config.regions)} regions available\")\n",
    "        print(f\"   Data Acquisition: {'‚úÖ Ready' if self.data_acquisition.authenticated else '‚ùå Not authenticated'}\")\n",
    "        print(f\"   Data Processor: {len(self.data_processor.analysis_scales)} analysis scales\")\n",
    "        print(f\"   AI Analyzer: Casarabe knowledge base loaded\")\n",
    "        print(f\"   Results Manager: Checkpoint 2 compliance monitoring\")\n",
    "        \n",
    "        # Data status\n",
    "        if self.loaded_data:\n",
    "            print(f\"\\nüìä Loaded Data:\")\n",
    "            for region_id, data in self.loaded_data.items():\n",
    "                print(f\"   {region_id}: {data['region_info']['name']}\")\n",
    "                print(f\"      Optical scenes: {data['metadata']['optical_scenes']}\")\n",
    "                print(f\"      Radar source: {data['metadata']['radar_source']}\")\n",
    "        \n",
    "        # Processing status\n",
    "        if self.processed_data:\n",
    "            print(f\"\\nüî¨ Processing Results:\")\n",
    "            for region_id, results in self.processed_data.items():\n",
    "                print(f\"   {region_id}: {results['region_name']}\")\n",
    "                print(f\"      Candidates: {len(results['discovery_candidates'])}\")\n",
    "                print(f\"      Analysis scales: {len(results['scales'])}\")\n",
    "        \n",
    "        # AI status\n",
    "        if hasattr(self, 'ai_analyses') and self.ai_analyses:\n",
    "            print(f\"\\nü§ñ AI Analysis:\")\n",
    "            total_prompts = sum(len(prompts) if isinstance(prompts, list) else 1 \n",
    "                              for prompts in self.ai_analyses.values() if prompts)\n",
    "            print(f\"   Total prompts: {total_prompts}\")\n",
    "            print(f\"   Discoveries: {len(self.ai_analyzer.discoveries)}\")\n",
    "            print(f\"   Leverage analysis: {'‚úÖ' if self.ai_analyses.get('leverage') else '‚ùå'}\")\n",
    "        \n",
    "        # Submission status\n",
    "        if self.final_submission:\n",
    "            print(f\"\\nüì¶ Submission:\")\n",
    "            print(f\"   Status: {self.final_submission['validation']['overall_status']}\")\n",
    "            print(f\"   Footprints: {len(self.final_submission['anomaly_footprints'])}\")\n",
    "            print(f\"   Quality score: {self.final_submission['quality_metrics']['average_discovery_confidence']:.3f}\")\n",
    "    \n",
    "    def show_final_validation_summary(self):\n",
    "        \"\"\"Show final validation summary\"\"\"\n",
    "        if not self.final_submission:\n",
    "            return\n",
    "        \n",
    "        validation = self.final_submission['validation']\n",
    "        \n",
    "        print(f\"üîç FINAL VALIDATION SUMMARY\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"üìä Overall Status: {validation['overall_status']}\")\n",
    "        print()\n",
    "        \n",
    "        # Check each requirement\n",
    "        requirements = validation['requirements']\n",
    "        \n",
    "        req_names = {\n",
    "            'independent_sources': 'Two Independent Data Sources',\n",
    "            'anomaly_footprints': 'Five Anomaly Footprints',\n",
    "            'dataset_logging': 'Dataset IDs Logged',\n",
    "            'prompt_logging': 'OpenAI Prompts Logged',\n",
    "            'reproducibility': 'Reproducibility Verified',\n",
    "            'leverage_analysis': 'Discovery Leverage'\n",
    "        }\n",
    "        \n",
    "        for req_key, req_name in req_names.items():\n",
    "            if req_key in requirements:\n",
    "                status = requirements[req_key]['status']\n",
    "                emoji = \"‚úÖ\" if status == 'PASS' else \"‚ùå\"\n",
    "                print(f\"{emoji} {req_name}: {status}\")\n",
    "        \n",
    "        if validation.get('critical_issues'):\n",
    "            print(f\"\\n‚ùå Critical Issues:\")\n",
    "            for issue in validation['critical_issues']:\n",
    "                print(f\"   ‚Ä¢ {issue}\")\n",
    "        \n",
    "        if validation.get('warnings'):\n",
    "            print(f\"\\n‚ö†Ô∏è Warnings:\")\n",
    "            for warning in validation['warnings']:\n",
    "                print(f\"   ‚Ä¢ {warning}\")\n",
    "        \n",
    "        print(f\"\\nüéØ Ready for competition submission!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0f88b41-97b4-4577-b1de-133e5f65a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main entry point for the enhanced archaeological discovery system\n",
    "    \"\"\"\n",
    "    print(\"üèõÔ∏è Enhanced Amazon Archaeological Discovery System\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üéØ OpenAI to Z Challenge - Checkpoint 2 Complete Solution\")\n",
    "    print()\n",
    "    print(\"This system implements:\")\n",
    "    print(\"‚Ä¢ Multi-scale analysis (50km ‚Üí 10km ‚Üí 2km)\")\n",
    "    print(\"‚Ä¢ Dual-source satellite data (Optical + Radar)\")\n",
    "    print(\"‚Ä¢ Archaeological knowledge integration (Casarabe culture)\")\n",
    "    print(\"‚Ä¢ Full Checkpoint 2 compliance validation\")\n",
    "    print(\"‚Ä¢ Reproducible methodology with ¬±50m tolerance\")\n",
    "    print()\n",
    "    \n",
    "    # Create the main system\n",
    "    system = EnhancedAmazonArchaeology()\n",
    "    \n",
    "    print(f\"üöÄ How do you want to proceed?\")\n",
    "    print(f\"1. Run complete pipeline automatically (recommended)\")\n",
    "    print(f\"2. Step-by-step interactive mode (for learning/debugging)\")\n",
    "    print(f\"3. Show system status only\")\n",
    "    print(f\"0. Exit\")\n",
    "    \n",
    "    try:\n",
    "        choice = input(f\"\\nSelect option (0-3): \").strip()\n",
    "        \n",
    "        if choice == '1':\n",
    "            # Complete automatic pipeline\n",
    "            regions = input(\"How many regions to analyze? (1-5, recommended: 3): \").strip()\n",
    "            max_regions = int(regions) if regions.isdigit() and 1 <= int(regions) <= 5 else 3\n",
    "            \n",
    "            print(f\"\\nüöÄ Starting complete pipeline with {max_regions} regions...\")\n",
    "            print(\"‚è±Ô∏è This will take 15-30 minutes depending on your internet connection\")\n",
    "            print(\"üîß Make sure you have:\")\n",
    "            print(\"   ‚Ä¢ Google Earth Engine access\")\n",
    "            print(\"   ‚Ä¢ OpenAI API key in keyring\")\n",
    "            print(\"   ‚Ä¢ Stable internet connection\")\n",
    "            \n",
    "            confirm = input(\"\\nProceed? (y/N): \").strip().lower()\n",
    "            if confirm == 'y':\n",
    "                success = system.run_complete_pipeline(max_regions)\n",
    "                \n",
    "                if success:\n",
    "                    print(f\"\\nüéâ Analysis completed successfully!\")\n",
    "                    print(f\"üìÅ Check these files:\")\n",
    "                    print(f\"   ‚Ä¢ checkpoint2_submission_*.json\")\n",
    "                    print(f\"   ‚Ä¢ checkpoint2_summary_*.md\") \n",
    "                    print(f\"   ‚Ä¢ enhanced_images/ folder\")\n",
    "                else:\n",
    "                    print(f\"\\n‚ùå Analysis failed. Check error messages above.\")\n",
    "            else:\n",
    "                print(\"üëã Operation cancelled\")\n",
    "        \n",
    "        elif choice == '2':\n",
    "            # Interactive step-by-step mode\n",
    "            system.run_step_by_step_mode()\n",
    "        \n",
    "        elif choice == '3':\n",
    "            # Show status only\n",
    "            system.show_detailed_status()\n",
    "        \n",
    "        elif choice == '0':\n",
    "            print(\"üëã Goodbye!\")\n",
    "        \n",
    "        else:\n",
    "            print(\"‚ùå Invalid choice\")\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nüëã Goodbye!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error: {e}\")\n",
    "        print(\"üîß Please report this issue\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a351daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèõÔ∏è Enhanced Amazon Archaeological Discovery System\n",
      "============================================================\n",
      "üéØ OpenAI to Z Challenge - Checkpoint 2 Complete Solution\n",
      "\n",
      "This system implements:\n",
      "‚Ä¢ Multi-scale analysis (50km ‚Üí 10km ‚Üí 2km)\n",
      "‚Ä¢ Dual-source satellite data (Optical + Radar)\n",
      "‚Ä¢ Archaeological knowledge integration (Casarabe culture)\n",
      "‚Ä¢ Full Checkpoint 2 compliance validation\n",
      "‚Ä¢ Reproducible methodology with ¬±50m tolerance\n",
      "\n",
      "üèõÔ∏è Enhanced Amazon Archaeological Discovery System\n",
      "============================================================\n",
      "üéØ OpenAI to Z Challenge - Checkpoint 2 Solution\n",
      "üî¨ Multi-scale Network Detection: 50km ‚Üí 10km ‚Üí 2km\n",
      "üìä Dual-source Analysis: Optical + Radar\n",
      "ü§ñ AI-powered with Archaeological Knowledge\n",
      "‚úÖ Full Checkpoint 2 Compliance\n",
      "\n",
      "üîß Initializing system components...\n",
      "üíæ Saved regions to simple_regions.json\n",
      "‚úÖ Loaded 5 regions from simple_regions.json\n",
      "üì° Enhanced Data Acquisition initialized\n",
      "üîÑ Dual-source capability: Optical + Radar\n",
      "üé® Enhanced Multi-Scale Processor initialized\n",
      "üìä Analysis scales: Regional (50km) ‚Üí Zone (10km) ‚Üí Site (2km)\n",
      "ü§ñ Enhanced Archaeological AI Analyzer initialized\n",
      "üèõÔ∏è Loaded Casarabe culture knowledge base\n",
      "üìä Scale-specific prompting ready\n",
      "üìä Enhanced Results Manager initialized\n",
      "üéØ Checkpoint 2 compliance monitoring active\n",
      "‚úÖ Reproducibility tolerance: ¬±50m\n",
      "‚úÖ All components initialized successfully!\n",
      "\n",
      "üöÄ How do you want to proceed?\n",
      "1. Run complete pipeline automatically (recommended)\n",
      "2. Step-by-step interactive mode (for learning/debugging)\n",
      "3. Show system status only\n",
      "0. Exit\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bff415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
